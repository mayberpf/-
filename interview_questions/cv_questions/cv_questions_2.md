# 训练中断了，如何继续训练？
这个我熟呀，因为之前在跑代码的时候，有一次加载不进来权重，所以就仔细的看了看权重文件到底是什么。在之前我们保存权重文件，只是通过很简单的一条命令，也就是只是将网络的参数保存起来。
```ruby
torch.save(model.state_dict(),"save_model/best_model.pth")
```
这一条命令就是简单的将模型的参数保存起来，最终得到的pth文件。那么问题来了，如果只是保存参数，那么训练中断了，岂不是就失忆了。所以想要在训练之后继续训练，首先我们一定会想到的一个参数epoch，也就是从第几个轮次失效，我们后面就继续从这个epoch训练。当然，复杂一点，还会存在别的信息。比如我设置了优化器，使我的学习率能够跟随迭代的轮次数进行迭代，那么我是不是也就需要将优化器的参数保存在pth文件中。这就造就了pth实际上也就是一个字典的格式。这样继续训练的话，就只需要把对应的参数加载进来就可以了。
我来找一个很容易理解的代码来进行说明。
```ruby
torch.save({
    'state_dict': net.state_dict(),
    'iteration': iteration,
    'epoch': epoch,
}, out_dir + '/checkpoint/%08d.model.pth' %  (iteration))
```
这种方式就很好的说明了我们实际上对pth构造成了一个字典，然后进行保存，因为在这个代码中的学习率是不变的，因此并没有保存优化器信息。
```ruby
f = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)
start_iteration = f['iteration']
start_epoch = f['epoch']
state_dict  = f['state_dict']
net.load_state_dict(state_dict,strict=False)  #True
```
后续在加载权重文件时候，我们只需要想加载字典一样，将对应的部分提取出来，然后继续训练就可以了。这只是理论部分，因为本人实际上没有对这个中断后继续训练进行实操过。

# 熟悉代码时如何更好的debug
这个问题似乎和cv无关！！！
起初我也是使用pycharm的，但是后来我发现pycharm很大，有时候开的多了，会很卡。后面我就换到了vscode。pycharm和vscode其实都自带断点的设置，所以调试只需要断点然后debug就好了。后面我学了一个新的debug方式，那就是使用pdb库。感觉还挺好使的。当然这些都是针对与Python的代码，对于c++，我只知道一点点东西把，好像是可以使用gdb进行debug，这个不太了解，后面的学习中可能会多接触c++啦。

# 如何查看模型的输出？
这些问题比较简单呀，如果面试真的全问这种问题，那不是so easy。
我们知道网络的模型一般都是写好的一个类，我们只需要将其实例化，然后给定模型一个输入，这个输入的大小，一般是4维度的张量(bs,channel,w,h)可以使用随机数，也可以加载图片然后扩展维度。我个人更喜欢加载图片，这样就可以直接可视化输出，看输出是什么东西。（除非输出是一些理论的数组，比如yolo的输出，就没有啥可视化的啦；但是对于图像分割很有用）。

# 如何释放GPU显存？有时候nvidia-smi看不到进程，但显卡占用比100%
nvidia-smi查看对应进程，然后将其杀死
```ruby
sudo fuser -v /dev/nvidia*#听说这个可以看僵尸进程
```
当然有时候我还会选择重启，哈哈哈哈哈
# 如何指定显卡训练
啧啧，把我问住了，因为我只有一张显卡，这部分没有了解。
查资料：
在代码中直接指定：
```ruby
import os
os.environ['CUDA_VISIBLE_DEVICES'] = gpu_ids
```
在命令行中执行代码时使用
```ruby
CUDA_VISIBLE_DEVICES = gpu_ids python main.py
```

# 如何判断是否是过拟合，如何解决？
实际上大多数情况判断过拟合，就是看训练集和验证集的loss值差距是不是很大。如果很大就有可能是过拟合。当然这里忽略了数据集分布的问题，那就是我们需要保证训练集和数据集的分布是大体差不多的。换句话说！你不能把简单的图片都给训练集，难的图片都给验证集。这天才也学不会呀。
那么如果真的过拟合了，可以试试更小的模型减少参数量。因为过拟合也可以理解为，学到了不该学到的东西。之前也从数学的方面对过拟合进行过分析。当然可以通过数据增强，正则化，bn，dropout等操作来抑制过拟合的发生。

# 注意力模块的原理是什么？
这个后续要深入了解！

# 测试集和验证集的区别？
工作中可以是一个，但是论文写作要分开。

# yolov2和yolov1中最后的类别误差都是使用的线性回归中的二次代价损失函数？而不是使用分类中的交叉熵函数，然而在yolov3之后用的都是交叉熵？
了解了解！！！