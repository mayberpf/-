# 15题
## 1、ReLU、tanh、sigmod的优缺点是什么？
## 2、为什么在RNN LSTM中，较多使用tanh，sigmod，而较少使用ReLU
## 3、回归任务，二分类任务，多分类任务，多标签类任务分别用什么损失函数
## 4、YOLO中为什么要使用smooth L1 Loss ，这么做的目的是什么
## 5、多任务loss如何平衡？

##### 1
这三个都是神经网络中的非线性激活函数。我觉着要说优缺点的话，首先就是要去看关于这三个激活函数的图像是怎样的，搞清楚他们可以把输入转换成怎样的输出。
一、sigmoid激活函数
公式：1/(1+exp(-w1x1+w2x2+…))
sigmoid函数是一条穿过(0,0.5)的s型曲线，上下限分别无限逼近1，0
#sigmoid函数的缺点：
1、倾向于梯度消失
2、执行指数运算，计算速度较慢
3、sigmoid函数输出不是以0为中心，而是以0.5为中心，会降低权重更新的速率

二、Tanh/双曲正切激活函数
公式：2/(1+exp(-2(w1x1+w2x2+…)))-1
Tanh函数是一条穿过(0,0)的s型曲线，上下限分别无限逼近1，-1

三、ReLU激活函数
公式：if(x<=0,0,x)
#ReLU函数的优点：
1、当输入为正时，不存在梯度饱和问题
2、ReLU 函数中只存在线性关系，因此它的计算速度比 sigmoid 和 tanh 更快
#缺点：
1、在反向传播过程中，如果输入负数，则梯度将完全为零(Dead ReLU 问题)
参考：http://t.csdn.cn/Vj3no

也就是：ReLU的优点在于计算量小，计算效率高，可以使神经网络快速收敛。但是缺点就是在于当输入为负数或者接近于0时就会使神经元失活，无法进行反向传播。tanh的优点在于值域以原点对称，输出均值是0，这样与很多样本的分布均值接近，同时也是一条平滑的曲线，可以防止输出值发生跳跃，缺点在于容易出现梯度消失，并且计算量大。sigmoid的优点也是一条平滑的曲线，和tanh一样，可以防止输出值发生跳跃，同时它的输出在0-1之间，可以对每个神经元的输出进行标准化，缺点和tanh类似，容易出现梯度消失以及计算量大，此外它的输出中心y值不是0

##### 2
RNN和LSTM都有门的机制，在一定程度上可以抑制梯度消失。内部使用sigmoid可以使神经元处于0-1的状态，这也是这些网络的要求。这种对称的激活函数可以决定保留哪些信息去除哪些信息。虽然tanh在连乘的过程中，也会出现梯度消失与梯度爆炸，但是使用tanh
的原因是因为比起sigmoid，tanh效果会好一些。

##### 3
回归任务：均方误差MSE、平均绝对误差MAE、smooth L1 loss，如果是边框回归还可以使用iou loss
二分类任务：0/1损失、二元交叉熵BCE、不平衡损失函数focal loss
多分类任务：分类交叉熵CE、focal  loss
多标签任务：二元交叉熵BCE


##### 4
smooth L1 loss可以使模型收敛的更快一些，预测框与gt差距大，梯度不会很大，差距小，梯度足够小。结合了L1 loss和L2 loss
的优点，在0的附近具备L2 loss 的优点：收敛更快且在点0有导数，在边界具备L1 loss的优点，可以让网络对异常值更具鲁棒性，发生较大误差还能拉回来。

这里我不太了解这三个损失函数，所以去查了资料，参考：http://t.csdn.cn/r8M6U
首先我们要明确这几个损失函数是针对目标检测哪一部分使用的。在之前的文章中，我们提到过目标检测，有关YOLO的loss函数实际上可以分为三大类：Location loss、Classes loss 、 Objectness loss 。其中Location loss针对bbox的定位是否准确，Classes loss针对预测分类是否准确，Objectness loss 针对预测框中是否有物体的判断是否准确。这一题中提到的三个损失函数是针对bbox的位置预测的。关于这一类的损失函数有一个演变过程：Smooth L1 Loss --> IoU Loss --> GIoU Loss --> DIoU Loss --> CIoU Loss
公式很简单，我们直接上图，就会明白为什么L1 loss的0点处不平滑、L2 loss在边界并不稳定的问题。
@import "smooth_l1_loss.png"


##### 5 
对多个loss进行量级调整？