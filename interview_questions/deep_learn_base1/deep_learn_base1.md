# 23
# 神经网络的层数是如何数的？
网络越深，模型效果越好，指的是可训练参数越多，模型的特征提取能力更好，所以我们其实可以知道：神经网络的层数只与可训练参数的层数有关。层数等于卷积层+全连接层数量。BN层，池化层，Flatten层都不算在内。可以对VGG-16进行分析，如图所示，vgg16的16层分别是2+2+3+3+3。这里最后一个3是3层全连接，其他层都是卷积层。
@import  "vgg.png"

#  卷积操作的作用是什么？（为什么通过堆叠卷积可以提取特征）
从单个卷积来说，作用很明显，卷积核参数是具有特定含义的矩阵，输入图像与该矩阵进行逐元素相乘，可以产生该矩阵所对应的输出。比如说：sobel算子，它的卷积核是[[1,0,-1],[2,0,-2],[1,0,-1]]。这个矩阵，将其与输入进行逐元素相乘，可得到该图像在竖直方向上的一阶梯度，该梯度对于边缘区域有较大的值，而非边缘区域则为0.因此通过这个卷积核可以提取图像竖直方向的边缘特征。这里是参考的资料中说竖直方向，但是我认为这样的卷积不应该是在水平方向求差异吗？
综上：卷积的参数不同时可以提取不同的特征，同时堆叠卷积的层数时，可以得到更大的感受野。
所以说整个神经网络实际上就是一个特征提取器，当卷积核的参数固定，不同类的图片经过神经网络就会得到不同的输出。所以卷积堆叠起来就可以提取更大范围的特征和更深层次的特征。

# 池化层的作用是什么？
池化层的操作是将一个窗口内的像素按照平均值加权或者选择最大值作为输出，一个窗口内仅有一个输出数据。因此，当经过池化层后，图像的尺寸会变小，计算量也会变小，相比于在池化前使用卷积，池化后同样的卷积大小具有更大的感受野。
但是对于仅仅缩小图像尺寸，减少计算量，获取更大感受野这个作用来说，也可以使用步长大于1的卷积来代替池化层。

但是池化还有其他作用。
最大池化除了前面所说的作用，还有一个去除冗余信息和去除噪声的作用。
《可视化一个训练过的卷积核》
根据这里的理论，每张图片经过同一个卷积核，会提取同一个类型的特征，而不符合这个特征的，输出的像素值就为零或者狠下，那么输入图片在经过这个待可视化的卷积核时，输出值越大，这说明越符合这个卷积核的所提取特征，然后我们利用梯度上升法，来调整像素值，不更新卷积核参数，以使该图片在经过待可视化卷积核后输出的像素值更大。当模型收敛后，此时的图片显示的就是该卷积核所认可的特征，从而达到可视化卷积核的目的。

除此之外，池化还具有平移不变性，旋转不变性，防止过拟合等作用

# 归一化层的作用
由于两个数据不再同一范围，但是他们使用相同的学习率，导致梯度下降轨迹沿一维来回震荡，从而需要更多的步骤才能达到最小值。具体的解释可以看这部分的有关bn的讲解。也就是如果不进行归一化，那么每层输入的分布实际上会随着每层的参数发生变化，这会导致训练变得复杂，随着网络的深入，网络参数的细微变化就会放的更大。

# 激活函数的作用
首先我们可能需要确定线性和非线性这两个概念，比如说全连接层实际上就是线性的。那么激活函数实际上就是非线性的。也就是神经网络实际上是对图像操作，将图像空间转换为特征空间。所以需要进行非线性映射。
# 损失函数的作用
用来衡量预测数据和真实数据之间的差距

# 全连接层的作用
全连接层的作用是对数据进行分类。第一种是图像通过卷积层提取了很多特征，第二种是将图像从像素空间映射到特征空间，在特征空间可以像在二位平面对两类数据进行分类那样。