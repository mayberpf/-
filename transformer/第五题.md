#第五题
###1、如何理解Transformer中的self-attention？2、如何理解multi-head？3、Transformer相比LSTM、RNN的优势在哪？4、Transformer的核心思想是什么？5、都说Transformer可以并行，它是怎么实现并行的？Encoder和Decoder都可以并行吗？
####个人见解
关于transformer前一段时间刚看过，有一点浅浅的了解。那是我第一次参加kaggle的一个比赛，是一个有关图像分割的比赛，通过那个比赛我初步了解了transformer并应用了swin相关的模型，虽然最后比赛的成绩不是很理想，道后面感觉到明显的乏力：精力不够，因为当时课题组还有任务、配置不高，我只有一张3080的显卡。具体情况我想着过些天总结以下。（跑题了）
我理解的transformer一开始是应用在nlp领域的一个模型，因为我不太懂，所以找了一些资料
```
#例如：分词、词嵌入、新词发现、拼写提示、词性标注、实体抽取、关系抽取、事件抽取、实体消歧、公指消解、文本分类、机器翻译、自动摘要、阅读理解等等，都是常见的 NLP 任务。
```
我当时看过一篇文章：https://zhuanlan.zhihu.com/p/48508221
讲的很详细，我的理解是，一句话若干个单词分开，进入到encoder和decoder中，其中encoder包括self-attention和feed-forward，decoder包括self-attention、feed-forward和encoder-decoder-attention。
我对self-attention的理解就是：在self-attention中，每个单词有3个不同的向量，它们分别是Query向量（Q），Key向量（K）和Value向量（V），长度均是64。它们是通过3个不同的权值矩阵由嵌入向量 X乘以三个不同的权值矩阵wq,Wv,Wk得到，其中三个矩阵的尺寸也是相同的。均是512*64。
详细步骤：
1、将输入的单词转化为嵌入向量
2、根据嵌入向量得到q、k、v三个向量
3、为每个向量计算一个score：score=q·k
4、为了梯度的稳定，transformer使用了score归一化
5、对score施以softmax激活函数
6、softmax点乘value值v，得到加权的每个输入向量的评分v
7、相加之后得到最终的输出结果z:z = v的总和
@import "1.jpg " {width="600px" height="800px" title="self-attention" alt="self-attention" }
当然在实际计算过程中，采用的是矩阵的形式，并且在self-attention中，还加入了short-cut残差结构，详细可以参考上面那篇文章。

多头注意力机制我理解实际上就是对多个self-attention的组装融合，本来一个输入词只有一个输出矩阵Z，但是多头的话，就会有多个Z，那么将Z拼接，再连接一个全连接层，最后得到唯一的输出矩阵Z。
关于和rnn及lstm的比较，可以参考：http://t.csdn.cn/9aivf
关于并行计算以及encoder和decoder的并行计算可以参考：https://zhuanlan.zhihu.com/p/368592551



