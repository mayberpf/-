#第三题
###设置学习率的本质是通过什么？如何对网络不同层设置不同学习率？
####个人见解
模型反向传播时，将loss转换为梯度，梯度和学习率改变训练中的各种权重。所以学习率的本质实际上是模型根据梯度学习改变权重的文件的大小。对于改变学习率这件事，本人只通过选择不同的优化器模型进行更改。
####资料学习
将输出误差反向传播给网络参数，以此来拟合样本的输出。本质上是最优化的一个过程，逐步趋向于最优解。但是每一次更新参数利用多少误差，就需要通过一个参数来控制，这个参数就是学习率（Learning rate）,也称为步长。因此学习率越大，参数更新越快，但是学习率如果过快就会导致模型的不收敛或者震荡。
学习率的影响
<table>
    <tr>
        <td>None</td> 
        <td>学习率大</td> 
        <td>学习率小</td> 
   </tr>
    <tr>
  		<td>学习速度</td> 
        <td>快</td> 
        <td>慢</td> 
    </tr>
    <tr>
        <td>使用时间点</td> 
        <td>刚开始训练</td> 
        <td>一定轮次之后</td> 
    </tr>
        <td>副作用</td> 
        <td>1.易损失值爆炸；2.易振荡。</td> 
        <td>1.易过拟合；2.收敛速度慢。</td> 
    </tr>
</table>
以上参考：http://t.csdn.cn/od1ri
关于学习率的调整，参考：https://zhuanlan.zhihu.com/p/435669796
关于学习率的调整首先要了解优化器，个人理解这是pytorch封装好的一个有一个类吧，不同的优化器有不同的学习策略。优化器的使用方法：
```ruby
loss.backward()
optimizer.step()
optimizer.zero_grad()
```
loss.backward()就是反向计算出各参数的梯度，然后optimizer.step()更新网络中的参数，optimizer.zero_grad()将这一轮的梯度清零，防止其影响下一轮的更新。(这里的pytorch需要梯度清零，其实还有一个原因就是：可以通过设定把不同的batch的梯度清零来实现模型的大batch_size训练)
常用的优化器，及优化器的引用方式
```ruby
import torch.optim.Adam
import torch.optim.SGD 
```
Optimizer基本属性
所有Optimizer公有的一些基本属性：
```ruby
lr: learning rate，学习率
eps: 学习率最小值，在动态更新学习率时，学习率最小不会小于该值。
weight_decay: 权值衰减。相当于对参数进行L2正则化（使模型复杂度尽可能低，防止过拟合），该值可以理解为正则化项的系数。
```
每个Optimizer都维护一个param_groups的list，该list中维护需要优化的参数以及对应的属性设置。

Optimizer基本方法
add_param_group(param_group)：为optimizer的param_groups增加一个参数组。这在微调预训练的网络时非常有用，因为冻结层可以训练并随着训练的进行添加到优化器中。
load_state_dict(state_dict)：加载optimizer state。参数必须是optimizer.state_dict()返回的对象。
state_dict()：返回一个dict，包含optimizer的状态：state和param_groups。
step(closure)： 执行一次参数更新过程。
zero_grad()： 清除所有已经更新的参数的梯度。
我们在构造优化器时，最简单的方法通常如下：
```ruby
model = Net()
optimizer_Adam = torch.optim.Adam(model.parameters(), lr=0.1) 
```
model.parameters()返回模型的全部参数，并将它们传入Adam函数构造出一个Adam优化器，并设置 learning rate=0.1。
因此该 Adam 优化器的 param_groups 维护的就是模型 model 的全部参数，并且学习率为0.1，这样在调用optimizer_Adam.step()时，就会对model的全部参数进行更新。

param_groups
Optimizer的param_groups是一个list，其中的每个元素都是一组独立的参数，以dict的方式存储。结构如下：

-param_groups    
    -0(dict)  # 第一组参数        
        params:  # 维护要更新的参数        
        lr:  # 该组参数的学习率        
        betas:        
        eps:  # 该组参数的学习率最小值        
        weight_decay:  # 该组参数的权重衰减系数        
        amsgrad:      
    -1(dict)  # 第二组参数    
    -2(dict)  # 第三组参数    
    ...
这样可以实现很多灵活的操作，比如：

####只训练模型的一部分参数
例如，只想训练上面的model中的layer1参数，而保持layer2的参数不动。可以如下设置Optimizer：
```ruby
model = Net()
#只传入layer层的参数，就可以只更新layer层的参数而不影响其他参数。
optimizer_Adam = torch.optim.Adam(model.layer1.parameters(), lr=0.1)  
```
2）不同部分的参数设置不同的学习率
例如，要想使model的layer1参数学习率为0.1，layer2的参数学习率为0.2，可以如下设置Optimizer：
```ruby
model = Net()
params_dict = [{'params': model.layer.parameters(), 'lr': 0.1},              
             {'params': model.layer2.parameters(), 'lr': 0.2}]
optimizer_Adam = torch.optim.Adam(params_dict)
```
这种方法更为灵活，手动构造一个params_dict列表来初始化Optimizer。注意，字典中的参数部分的 key 必须为‘params’。

####动态更新学习率
了解了Optimizer的基本结构和使用方法，接下来将向你介绍如何在训练过程中动态更新 learning rate。

1. 手动修改学习率
前文提到Optimizer的每一组参数维护一个lr，因此最直接的方法就是在训练过程中手动修改Optimizer中对应的lr值。
```ruby
model = Net()  # 生成网络
optimizer_Adam = torch.optim.Adam(model.parameters(), lr=0.1)  # 生成优化器
for epoch in range(100):  # 假设迭代100个epoch    
    if epoch % 5 == 0:  # 每迭代5次，更新一次学习率        
        for params in optimizer_Adam.param_groups:             
            # 遍历Optimizer中的每一组参数，将该组参数的学习率 * 0.9            
            params['lr'] *= 0.9            
            # params['weight_decay'] = 0.5  # 当然也可以修改其他属性
```
2. torch.optim.lr_scheduler
torch.optim.lr_scheduler包中提供了一些类，用于动态修改lr。

torch.optim.lr_scheduler.LambdaLr
torch.optim.lr_scheduler.StepLR
torch.optim.lr_scheduler.MultiStepLR
torch.optim.lr_scheduler.ExponentialLR
torch.optim.lr_sheduler.CosineAnneaingLR
torch.optim.lr_scheduler.ReduceLROnPlateau
pytorch 1.1.0版本之后，在创建了lr_scheduler对象之后，会自动执行第一次lr更新（可以理解为执行一次scheduler.step()）。

因此在使用的时候，需要先调用optimizer.step()，再调用scheduler.step()。

如果创建了lr_scheduler对象之后，先调用scheduler.step()，再调用optimizer.step()，则会跳过了第一个学习率的值。

调用顺序
loss.backward()
optimizer.step()
scheduler.step()...
具体使用方法由于篇幅有限不在此阐述了，感兴趣的伙伴可以去torch官网查看文档。


