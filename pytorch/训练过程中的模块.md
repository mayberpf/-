# 训练日志
一开始接触我们都会觉着log没有什么用，只要代码跑通了就好了，但是到后面我们会发现，log日志才是神经网络中最有用的东西之一，除了结果可视化，log文件应该是第二个能反应训练情况的东西了。
首先是准备好基本的配置
```ruby
import logging
def train_logger(num):
logger = logging.getLogger(__name__)
#设置打印的级别,一共有6个级别,从低到高分别为:
#NOTEST、DEBUG、INFO、WARNING、ERROR、CRITICAL。
#setLevel设置的是最低打印的级别,低于该级别的将不会打印。
logger.setLevel(level=logging.INFO)
#打印到文件,并设置打印的文件名称和路径
file_log = logging.FileHandler('./run/{}/train.log'.format(num))
#打印到终端
print_log = logging.StreamHandler()
#设置打印格式
#%(asctime)表示当前时间,%(message)表示要打印的信息,用的时候会介绍。
formatter = logging.Formatter('%(asctime)s
%(message)s')
file_log.setFormatter(formatter)
print_log.setFormatter(formatter)
logger.addHandler(file_log)
logger.addHandler(print_log)
return logger
```
使用方法
```ruby
logger = train_logger(0)
logger.info("project_name: {}".format(project_name))
logger.info("batchsize: {}".format(opt.batchsize))
logger.warning("warning message")
```

# 随机种子数
随机种子数是为了固定数据集每次的打乱顺序，让模型变得可复现。不设置随机数种子的话，由于数据集每次打乱的顺序不一样，导致模型会略有浮动。
模型浮动的原因：
数据集随机打乱，导致batch不同！
目前基本都是使用mini-batch梯度下降,也就是说每次都是前传一个batch的数据后,才会更新权重,与此同时,模型基本都是有使用BN,即对每个batch做归一化。因此,batch数据对模型的性能会有一定的影响。如果每次随机顺序都不一样,可能会存在某几次的batch组合得非常好,以至于模型训练效果不错,而其它时候的batch的组合不是很合适,以至于达不到组合得很好的时候的效果。因此,所谓的原因就是每次的不同顺序产生了batch样本的多样性,batch样本多样性对模型的结果有一定的影响。数据集越小,这个影响可能越大,因为造成的batch样本之间的差异性很大,而数据集越大时,batch样本之间的差异性可能受到随机顺序的影响越小。因此,设置随机数种子时一个必要的事情。计算机上的随机数都是人工模拟出来的,因此,我们可以任意地设置随机数的范围等。
使用方法:
```ruby
import random
#基本配置
def setup_seed(seed):
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.cuda.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)
cudnn.deterministic = True
#使用方法
#在train文件下调用下面这行命令,参数随意设置,
#只要这个参数数值一样,每次生成的顺序也就一样
setup_seed(2022)
```

# classdataset初始化
这一部分比较简单，也就是每个模型都会有的，说白了就是dataset实例化，然后调用DataLoader就好了。
```ruby
from torch.utils.data import DataLoader
from dataloader import MyDataset
train_folder = opt.data_dir + '/train'
train_dataset = MyDataset(data_folder=train_folder,opt=opt)
train_loader = DataLoader(train_dataset, batch_size=opt.batchsize,
shuffle=True, num_workers=8)
#这里shuffle表示是否打乱数据，默认为True，num_workers读取数据使用的进程数。
#下面这里是使用多机多卡进行分布式训练的设定，此时shuffle=FALSE，但是要给定sampler，这是一个样本索引的提取器。
from torch.utils.data.distributed import DistributedSampler
train_sampler =
torch.utils.data.distributed.DistributedSampler(train_dataset)
train_loader = DataLoader(train_dataset, batch_size=opt.batchsize,
sampler=train_sampler)
```

# 网络的初始化
网络初始化也很简单，就是对网络的类进行实例化就好了，最多在加载一个预训练模型,并将该网络放入GPU
```ruby
from model import Yolo_v1
net = Yolo_v1()
net.load_state_dict(torch.load(trained_path))
net = net.cuda()
```

# 学习率设置
这里主要弄清楚一个事情，那就是不同层设定不同的学习率。
```ruby
import torch.optim as optim
from torch.optim import lr_scheduler
optim_params = [{'params': net.backbone.parameters(), 'lr': 0.1 * opt.lr},
{'params': net.interaction.parameters(), 'lr': opt.lr},
{'params': net.large_conv1_1.parameters(), 'lr': opt.lr},
{'params': net.large_conv1_2.parameters(), 'lr': opt.lr},
{'params': net.classifier.parameters(), 'lr': opt.lr}
]
optimizer = optim.SGD(optim_params, weight_decay=5e-4, momentum=0.9,
nesterov=True)
#设置参数和对应学习率的列表，然后作为参数传给optim优化器。
#设置学习率调度器lr_scheduler，用于训练到不同的epoch时调整学习率。这里的step_size表示在每80个epoch时，学习率乘以gamma
scheduler = lr_scheduler.StepLR(optimizer, step_size=80, gamma=0.1)
```
还可以通过使用```lr_scheduler.MultiStepLR(optimizer, milestones=[30,50,60], gamma=0.1, last_epoch=-1)```设置学习率的改变，这个例子是30，50，60epoch学习率发生改变。
学习率的调整应该放在optim优化器更新之后（pytorch1.1.0版本后）
```ruby
for epoch in range(opt.num_epochs):
    #省略一部分代码
    loss.backward()
    optimizer.step()
scheduler.step()
```
# 损失函数的设置
我们很多时候在使用损失函数，都是直接调用的，那么直接调用的损失函数的使用很简单。
```ruby
import torch.nn as nn
cls_criterion = nn.CrossEntropyLoss()
dist_criterion = nn.MSELoss()
# Use L2 loss function
hinge_criterion = nn.HingeEmbeddingLoss()
#这里调用实例化之后，直接给它预测值和真实值，得到loss，然后进行反向传播即可。
```
但是很多时候我们会自己设计loss函数
实际上自己设计loss和网络定义差不多，继承nn.Module然后完成init和forward函数即可。损失函数中没有可训练的参数，因此通常直接使用torch.nn.functional中的函数即可。
注意！torch.nn.functional中的函数和nn中的函数是有区别的！前者需要自己设置权重，且不会随训练过程更新，后者不需要自己设置权重，权重会更新
重点了解
```ruby
import torch
loss = torch.nn.BCELoss()
loss_2 = torch.nn.functional.binary_cross_entropy()
```

# tensorboard配置
使用方法
```ruby
from torch.utils.tensorboard import SummaryWriter
logdir = 'log'
writer = SummaryWriter(log_dir=logdir)
for epoch in range(opt.num_epochs):
train_acc = ...
loss = ...
writer.add_scalar('train/train_acc', train_acc, epoch+1)
writer.add_scalar('train/loss', loss.item(), epoch+1)
writer.close()
```
其中add_scalar主要有三个参数
tag：标签
scalar_value：标签的值
global_step：标签的x轴坐标
每使用一个add_scalar就会多一个坐标图，如果有条曲线想画在同一个坐标图上，则可以使用字典的形式。
```ruby
writer.add_scalars('Training',
{'train_acc': train_acc,
'val_acc': val_acc},
epoch+1)
```
当然除了画曲线，还可以画别的.例如：write.add_graph(),write.add_image()
显示就是使用命令```tensorboard --logdir = 'log'```

# 训练过程搭建
我们可以直接看代码了解整个训练过程需要哪些部分
```ruby
def train(net, criterion, optimizer, scheduler, train_loader, val_loader,
writer):
    for epoch in range(opt.num_epochs -start_epoch):
    #设置start_epoch是因为有时是从恢复训练,不是从零开始的。
        epoch = epoch + start_epoch
        print('Epoch {}/{}'.format(epoch+1, opt.num_epochs))
        print('-' * 10)
        print('lr:{}'.format(optimizer.param_groups[0]['lr']))
        net.train(True)#这一行没写成为很多人训练失败的原因
        train_acc = 0.0
        train_loss = 0.0
        for i, (img_data, label) in enumerate(train_loader):
            if img_data.shape[0] < opt.batchsize:
            # skip the last batch
                continue
            img_data = Variable(img_data.cuda())
            label = Variable(label.cuda())
            optimizer.zero_grad()
            # forward
            output= net(img_data )
            loss = criterion(pred,label)
            train_loss += loss.data[0]
            pred = output.argmax(dim=1)
            train_acc += float(torch.sum(pred == label.data))
            loss.backward()
            optimizer.step()
        #学习率调整
        scheduler.step()
        #计算准确率,计算损失
        train_acc = train_acc / len(train_loader)
        train_loss = train_loss / len(train_loader)
        writer.add_scalar('Train_loss', train_loss, global_step=epoch)
        writer.add_scalar('Train_acc', train_acc, global_step=epoch)
        if (epoch +1)%5 == 0:
            torch.save({'epoch': epoch,
            'optimizer_dict': optimizer.state_dict(),
            'model_dict': model.state_dict()},
            save_path)
        #验证集
        val(net, criterion, val_loader, writer, epoch)
    writer.close()
def val(net, criterion, val_loader, writer, epoch):net = net.eval()
    val_acc = 0.0
    val_loss = 0.0
    with torch.no_grad():
        for i, (img_data, label) in enumerate(val_loader):
            if img_data.shape[0] < opt.batchsize:
            # skip the last batch
                continue
            img_data = Variable(img_data.cuda())
            label = Variable(label.cuda())
            # forward
            output= net(img_data )
            loss = criterion(pred,label)
            val_loss += loss.data[0]
            pred = output.argmax(dim=1)
            val_acc += float(torch.sum(pred == label.data))
        #计算准确率,计算损失
        val_acc = val_acc / len(val_loader)
        val_loss = val_loss / len(val_loader)
        writer.add_scalar('Val_loss', val_loss, global_step=epoch)
        writer.add_scalar('Val_acc', val_acc, global_step=epoch)
```

# 断点训练
这个可能是我们经常使用的啦
首先来了解一下模型的保存
```ruby
torch.save({'epoch': epoch,
    'optimizer_dict': optimizer.state_dict(),
    'model_dict': model.state_dict()},
    save_path)
```
当我们以这种方式进行保存，后续就可以通过下面的函数来读取模型，包括epoch等参数
```ruby
def load_model(save_name, optimizer, model):
    model_data = torch.load(save_name)
    start_epoch = model_data['epoch']
    model.load_state_dict(model_data['model_dict'])
    optimizer.load_state_dict(model_data['optimizer_dict'])
    print("model load success")
    return start_epoch, model, optimizer
```
在train文件中，加上一个调用上面函数的代码，即可
```ruby
if opt.resume:
    load_model(save_name,optimizer,model)
```