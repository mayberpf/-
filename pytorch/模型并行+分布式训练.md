# 模型并行+分布式训练
想总结这一部分的原因，主要是之前我记得torch.nn.DataParalle是在有多张GPU时才会使用的，但是最近看了一个代码，就是只有一个GPU，但是还是用到了torch.nn.DataParalle。模型中有这几行代码，我不太能理解
```ruby
        model_train = torch.nn.DataParallel(model)
        cudnn.benchmark = True
        model_train = model_train.cuda()
```
# DP
首先我们来了解一下nn.DataParallel(DP)
DP使用起来很方便，只需要将原来单卡的module用dp改成多卡即可，使用教程如下：
```ruby
import torch
import torch.distributed as dist
# 设置使用的gpu数
gpus = [0, 1, 2, 3]
torch.cuda.set_device('cuda:{}'.format(gpus[0]))
# 定义dataset
train_dataset = ...
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=...)
# 定义model
model = ...
# 使用nn.DataParallel包装模型
model = nn.DataParallel(model.to(device), device_ids=gpus, output_device=gpus[0])
optimizer = optim.SGD(model.parameters())
for epoch in rangimport torch
import torch.distributed as dist
# 设置使用的gpu数
gpus = [0, 1, 2, 3]
torch.cuda.set_device('cuda:{}'.format(gpus[0]))
# 定义dataset
train_dataset = ...
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=...)
# 定义model
model = ...
# 使用nn.DataParallel包装模型
model = nn.DataParallel(model.to(device), device_ids=gpus, output_device=gpus[0])
optimizer = optim.SGD(model.parameters())
for epoch in range(100):
for batch_idx, (data, target) in enumerate(train_loader):
images = images.cuda(non_blocking=True)
target = target.cuda(non_blocking=True)
...
output = model(images)
loss = criterion(output, target)
...
optimizer.zero_grad()
loss.backward()
optimizer.step()cuda(non_blocking=True)
target = target.cuda(non_blocking=True)
...
output = model(images)
loss = criterion(output, target)
...
optimizer.zero_grad()
loss.backward()
optimizer.step()
```
这个代码看上去和单卡大体上差不多，主要就是就将GPU的编号放在一个列表里面，然后将模型分别拷入到不同的GPU
原理部分，我在看了资料之后想起之前我肯定也做过这一方面的学习，也就是在使用nn.DataParalle时，虽然是将batchsize平分给各个显卡，但是是主显卡负责梯度的整合和更新。
具体的源码分析，我没有看，但是有资料去讲解的。但是我们可以看到，很明显一点就是负载是不平衡的。于是就出现了torch.nn.parallel.DistributedDataParallel(DP)。
# DDP
首先我们先看一下它和DP的区别
多进程，DDP采用多进程，最推荐的做法是每张卡上一个进程从而避免上一节所说单进程带来的影响。
通信效率，DP的通信成本随着GPU数量线性增长，而DDP支持Ring AllReduce，其通信成本是恒定的，与GPU无关
同步参数：DP通过收集梯度到主显卡也就是device[0]，在device[0]更新参数，然后其他设备复制device[0]的参数实现各个模型同步，DDP通过保证初始状态下相同并且改变量也相同(同步梯度)，保证模型同步。
接下来看一下如何实现DDP
```ruby
        import torch
        import argparse
        import torch.distributed as dist
        parser = argparse.ArgumentParser()
        parser.add_argument('--local_rank', default=-1, type=int,
        help='node rank for distributed training')
        args = parser.parse_args()
        # 初始化GPU通信方式(NCCL)和参数的获取方式(env代表通过环境变量)。
        dist.init_process_group(backend='nccl', init_method='env://')
        # 在启动器为我们启动python脚本后,在执行过程中,启动器会将当前进程的(其实就是 GPU的)index
        #通过参数传递给 python,我们可以这样获得当前进程的 index:即通过参数 local_rank 来告诉我们当
        #前进程使用的是哪个GPU,用于我们在每个进程中指定不同的device:
        torch.cuda.set_device(args.local_rank)
        train_dataset = ...
        # 在读取数据的时候,我们要保证一个batch里的数据被均摊到每个进程上,每个进程都能获取到不同的
        #数据,但如果我们手动去告诉每个进程拿哪些数据的话太麻烦了,PyTorch也为我们封装好了这一方法。
       # 之后,使用 DistributedSampler 对数据集进行划分。如此前我们介绍的那样,它能帮助我们将每个
        #batch 划分成几个 partition,在当前进程中只需要获取和 rank 对应的那个 partition 进行训练。
        #所以我们在初始化 DataLoader 的时候需要使用到
       #torch.utils.data.distributed.DistributedSampler 这个特性:
        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=...,
        sampler=train_sampler)
        # 使用 DistributedDataParallel 包装模型,它能帮助我们为不同 GPU 上求得的梯度进行 all
        #reduce(即汇总不同 GPU 计算所得的梯度,并同步计算结果)。all reduce 后不同 GPU 中模型的梯
        #度均为 all reduce 之前各 GPU 梯度的均值。
        model = ...
        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank])
        optimizer = optim.SGD(model.parameters())
        for epoch in range(100):
        for batch_idx, (data, target) in enumerate(train_loader):
        images = images.cuda(non_blocking=True)
        target = target.cuda(non_blocking=True)
        ...
        output = model(images)
        loss = criterion(output, target)
        ...
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

我个人感觉，其实这一方面就需要真正的实践才能彻底掌握。