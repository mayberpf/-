# 模型冻结、微调
之前在学习别人代码的时候偶然了解过一些关于网络冻结的事情，这里我们来详细的学习一下。
首先我们应该了解一下：迁移学习。简单讲，迁移学习就是将别的领域学到的知识，迁移到别的新的领域中。举个例子：不论是目标检测还是图像分割，我们知道都会有一个backbone进行特征提取，也就是说，backbone做的实际上就是对图像的一些点线纹理等特征进行提取。所以说，在这些方面实际上是有一些共性，那么这些共性，就是我们在训练初期不太想变的，所以就出现了网络冻结。
## 模型微调
首先我们看下模型微调。模型微调就是在别人已经训练好的模型上，将个别层的参数进行重新设置，然后训练，使其能够满足我们的应用场景。举个例子，我们找到一个在imagenet上训练的预训练模型，然后使用在我们自己的场景，因为imagenet的类别数是1000，也就是在模型最后一层使用的可能是一个输出为1000的线性层，但是我们自己的场景只有10个类别数，所以我们需要更改下最后的线性层。同时因为在预训练中模型已经学到了一些浅层的特征提取信息，所以我们会将前面的网络层进行冻结，然后只训练后面个别层，这样可以使模型更快的收敛。
简单说：为什么要用预训练模型做微调呢？
因为预训练模型用了大量的数据做训练，已经具备了提取浅层基础特征和深层抽象特征的能力。
需要我们注意的是：在进行模型微调的时候，应使用较小的学习率。因为预训练模型的权重相对于随机初始化的权重来说已经很棒了，所以不希望使用太大的学习率来破坏原本的权重。通常用于微调的初始学习率会比从头开始训练的学习率小10倍。
### 微调的情况
这里微调的不同情况根据我们自己应用场景的数据集数量和自身数据集与预训练数据集的相似程度进行区分。
数据少，相似程度高：可以只修改最后几层或者最后一层进行微调。
数据少，相似程度低，冻结预训练模型的前几层，训练剩余的层。因为数据集之间的相似程度低，所以根据自身的数据集对较高层进行重新训练会比较有效。
数据多，相似程度高，(最理想的情况)使用预训练的权重来初始化模型，然后重新训练整个模型。这是最简单的微调方式。
数据多，相似程度低，微调效果可能不好，可以考虑直接重新训练整个模型。
## 参数冻结
网络冻结，可以分为两类：全程冻结和非全程冻结。
全程冻结很好理解，就是模型的参数在整个训练过程都不会更新。
非全程冻结，就是在训练过程中存在一个解冻的过程，解冻之后，相应参数就可以学习更新啦。
为了了解模型如何冻结，我们首先应该知道模型是什么！
其实模型在读取之后，都是一个字典。所以想到冻结字典中的部分层，可以经过遍历，然后让对应需要冻结的层的一个参数设置为False
```ruby
requires_grad = False
```
第一种方式：
通过数字对模型进行遍历，冻结指定的若干层：
```ruby
count = 0 
for layer in model.children():
    count+=1
    if count < 10:
        for param in layer.parameters():
            param.requires_grad = False
```
然后我们还需要将训练的参数传入到优化器中，也就是优化器中不应该存在冻结的参数
```ruby
optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,model.parameters()), lr=LR)
```
第二种方式就是通过名字对模型进行遍历。
```ruby
if freeze_layers:
    for name, param in model.named_parameters():
    # 除最后的全连接层外,其他权重全部冻结
        if "fc" not in name:
            param.requires_grad_(False)
pg = [p for p in model.parameters() if p.requires_grad]
optimizer = optim.SGD(pg, lr=0.01, momentum=0.9, weight_decay=4E-5)
```
或者通过判断冻结参数在模型的哪些模块中，然后进行遍历
```ruby
if Freeze_Train:
    for param in model.backbone.parameters():
        param.requires_grad = False
```
后面简单说下模型修改
有多种方式：先加载权重后修改、先修改后选择性加载权重。
```ruby
import torch.nn as nn
import torch
class AlexNet(nn.Module):
    def __init__(self):
        super(AlexNet, self).__init__()
        self.features=nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),
            # 使用卷积层,输入为3,输出为64,核大小为11,步长为4
            nn.ReLU(inplace=True),
                #  使用激活函数
            nn.MaxPool2d(kernel_size=3, stride=2),
            # 使用最大池化,这里的大小为3,步长为2
            nn.Conv2d(64, 192, kernel_size=5, padding=2), # 使用卷积层,输入为64,输出为192,核大小为5,步长为2
            nn.ReLU(inplace=True),# 使用激活函数
            nn.MaxPool2d(kernel_size=3, stride=2), # 使用最大池化,这里的大小为3,步长为2
            nn.Conv2d(192, 384, kernel_size=3, padding=1), # 使用卷积层,输入为192,输出为384,核大小为3,步长为1
            nn.ReLU(inplace=True),# 使用激活函数
            nn.Conv2d(384, 256, kernel_size=3, padding=1),# 使用卷积层,输入为384,输出为256,核大小为3,步长为1
            nn.ReLU(inplace=True),# 使用激活函数
            nn.Conv2d(256, 256, kernel_size=3, padding=1),# 使用卷积层,输入为256,输出为256,核大小为3,步长为1
            nn.ReLU(inplace=True),# 使用激活函数nn.MaxPool2d(kernel_size=3, stride=2),
            # 使用最大池化,这里的大小为3,步长为2
)
        self.avgpool=nn.AdaptiveAvgPool2d((6, 6))
        self.classifier=nn.Sequential(
            nn.Dropout(),# 使用Dropout来减缓过拟合
            nn.Linear(256 * 6 * 6, 4096),
# 全连接,输出为4096
            nn.ReLU(inplace=True),# 使用激活函数
            nn.Dropout(),# 使用Dropout来减缓过拟合
            nn.Linear(4096, 4096), # 维度不变,因为后面引入了激活函数,从而引入非线性
            nn.ReLU(inplace=True), # 使用激活函数
            nn.Linear(4096, 1000),
#ImageNet默认为1000个类别,所以这里进行1000个类别分类
)
    def forward(self, x):
        x=self.features(x)
        x=self.avgpool(x)
        x=torch.flatten(x, 1)
        x=self.classifier(x)
        return x
def alexnet(num_classes, device, pretrained_weights=""):
    net=AlexNet()
    # 定义AlexNet
    if pretrained_weights:
        # 判断预训练模型路径是否为空,如果不为空则加载
        net.load_state_dict(torch.load(pretrained_weights,map_location=device))
    num_fc=net.classifier[6].in_features
    # 获取输入到全连接层的输入维度信息
    net.classifier[6]=torch.nn.Linear(in_features=num_fc,
    out_features=num_classes) # 根据数据集的类别数来指定最后输出的out_features数目
    return net
```
上面这种方法就是简单的先加载模型权重，然后再修改模型的最后一个层。