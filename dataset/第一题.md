#第一题
###如何理解加载数据集时的打乱顺序？他是如何完成的？若是分布式训练又是如何完成的？在打乱顺序的前提下每个epoch的batch都一样吗？如何理解DataLoader中的shuffle和sample？如何提高加载数据的速度？
我认为在加载数据集时，对数据进行打乱，主要是希望提升模型整体的泛化能力，举个例子：如果进行猫狗识别，并且没有对数据集打乱，那么labels可能是这样的
```r
Dog，Dog，Dog，... ，Dog，Dog，Dog，Cat，Cat，Cat，Cat，... ，Cat，Cat
```
那么网络模型在每一轮次，只要前面的预测为Dog后面的预测为Cat，那么正确率依然很高。
目前我用到的打乱数据的方式是通过DataLoader函数进行的
```r
DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
           batch_sampler=None, num_workers=0, collate_fn=None,
           pin_memory=False, drop_last=False, timeout=0,
           worker_init_fn=None, *, prefetch_factor=2,
           persistent_workers=False)
#dataset 数据集，map-style and iterable-style 可以用index取值的对象、
#batch_size 大小
#shuffle 取batch是否随机取， 默认为False
#sampler 定义取batch的方法，是一个迭代器， 每次生成一个key 用于读取dataset中的值
#batch_sampler 也是一个迭代器， 每次生次一个batch_size的key
#num_workers 参与工作的线程数
#collate_fn 对取出的batch进行处理
#drop_last 对最后不足batchsize的数据的处理方法

```
这里的sampler因为我没有使用过，根据查资料，sampler是一个取样策略。具体参考http://t.csdn.cn/9DsBo
PyTorch 中实现数据并行的操作可以通过使用。具体参加http://t.csdn.cn/8fmbx
```r
torch.nn.DataParallel
#forward：将输入的一个batch分为多份，送到多个GPU计算
#backward：主GPU收集网络输出，并计算损失值，将损失下发到各个GPU反向传播计算梯度，最后在主GPU上更新模型参数
torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=None, rank=None)
#不再有主GPU，每个GPU执行相同的任务
```
关于每个epoch的batch是否相同，我的理解是因为需要每个eopch都需要遍历一边数据，每次遍历数据又会将数据打乱，所以我认为每个eopch的batch的大小是一样的，因为这是我们设置的，但是每个batch所对应的内容应该是不一样的。
######PS：还有一种情况会导致batch大小不一样，那就是将数据集进行划分，但是剩余几个不足划分为一个batch，我们可以通过设置drop_last，若为True，则会丢弃不足的部分。FALSE的话，就多一个batch。

关于提高加载数据的方法：
参考https://zhuanlan.zhihu.com/p/66145913
Nvidia 是在读取每次数据返回给网络的时候，预读取下一次迭代需要的数据，因此
```r
data, label = prefetcher.next()
iteration = 0
while data is not None:
    iteration += 1
    # 训练代码
    data, label = prefetcher.next()
```
还有换一个固态硬盘
考虑DataLoader中可以设置num_workers来实现多线程加载数据。
DataLoader中设置pin_memory为True，这样所有的数据都是在锁页内存上，而不会放到虚拟内存上，这样数据都在内存上加载的，会快一些。锁页内存的内容都是在内存上而非锁页内存方式很可能会有一部分数据在虚拟内存（即硬盘）上，从硬盘上加载速度比较慢。
这里说的pin_memory没有做过，不太了解。