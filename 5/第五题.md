#第五题
###1、如何理解Transformer中的self-attention？2、如何理解multi-head？3、Transformer相比LSTM、RNN的优势在哪？4、Transformer的核心思想是什么？5、都说Transformer可以并行，它是怎么实现并行的？Encoder和Decoder都可以并行吗？
