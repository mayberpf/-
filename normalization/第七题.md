#第七题
###如何理解归一化？为什么要归一化?常见的归一化有哪些？它们的区别是什么？各自的优缺点、应用场合是什么？如何理解IN适用于GAN、风格迁移等领域？为什么transformer要用LN，BN为什么不行？GN的作用是什么？BN有哪些缺陷？
####个人见解
我想归一化的操作，我们在网路结构中经常见到，尤其是整个模块，也就是这样一整个代码块，我们经常看到：
```r
self.CBL = nn.Sequential(
    nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding),
    nn.BatchNorm2d(out_channels),
    nn.LeakyReLU()
)
```
就是将卷积卷积，bn，激活函数放在了一个容器里。这种结构经常的 出现。其中batchnorma就是归一化。在之前的学习中，了解到其实bn就是使特征图在模型中能够保证均值为0，方差为1的分布。并且一般都将bn层放置在卷积conv和激活函数relu之间；同时，设置的一个批次的大小也有助于bn发挥作用，即batchsize越大，bn效果越明显，但是batchsize=1时，bn基本没有什么效果了。
关于bn在很早就有一篇论文：https://arxiv.org/abs/1502.03167
####资料查询
参考：http://t.csdn.cn/rCPge
不同评价指标（即特征向量中的不同特征就是所述的不同评价指标）往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。其中，最典型的就是数据的归一化处理。
简而言之，归一化的目的就是使得预处理的数据被限定在一定的范围内（比如[0,1]或者[-1,1]），从而消除奇异样本数据导致的不良影响。
在上述的文章中也提到了有无归一化的对比，因此归一化的好处是：归一化后加快了梯度下降求最优解的速度，也即加快训练网络的收敛性；归一化有可能提高精度。
参考：http://t.csdn.cn/cyZkR
上面这篇文章讲解的很详细，有关bn是如何实现的
@import "1.png" 
图中是一个batch，这个batch包括了三个样本，每个样本的特征图有三个通道，那么bn实际上就是计算每个通道的均值和方差，然后做相应的归一化以及加入γ和β进行线性变换。比如：黄色通道的均值就是三个样本各自的黄色通道相加然后除以3(batch_size)*H*W,同理方差。
LN则是通过对每个样本求方差，均值，然后在做相应的归一化。不需要考虑batchsize的大小。因此------>我们不难想到，如果你的电脑性能不太好，不能拥有很大的batchsize的话，甚至batch为1、2，那么建议将网络中的bn换为ln，我还没有试过，但是在后面估计要尝试一下了。
IN计算的就i更小一层了，对每一个样本的每一个通道的H、W求均值和标准差，然后再做相应的归一化。
GN是介于LN和IN之间的一种归一化方式。它对每一个样本的通道数进行分组，同一组的通道一同进行均值和标准差的计算。具体的示意图可以看下参考链接。

不管是上面哪种归一化方式，通过均值和标准差进行归一化之后，都需要进行缩放变换，也就是乘以γ再加上β，即需要额外学习γ和β两个参数。其中对于BN，IN和GN，γ和β都是维度为通道数C的向量，相当于对初步归一化好的特征图进行每一个“像素点”下的C个通道统一变换；而对于LN，γ和β是特征图的矩阵大小，对初步归一化好的特征图的“每一片”进行变换。

至于transformer为什么采用ln而不是bn
参考：http://t.csdn.cn/r3GKo
实际上LSTM训练的时候我们会把一个batch中的序列按照长度降序，长度短的计算完了就不带它继续计算了，相当于batch size缩小了，batch size越小，BN的意义也就越小了。
在Transformer中也是一样的，比如我们用绝对位置编码的BERT，把序列长度都padding或者裁剪到512，那么不存在变长问题，也不存在LSTM中batch缩小的问题，那么为什么不用BN而是用LN呢？我的理解是因为虽然序列长度一致了，但是好多embedding是没有意义的，有意义的embedding不应该和它们的分布一致，如果BN会导致有意义的embedding损失信息，所以embedding你就自己和自己归一化吧。
所以，可以用BN，但是可能LN更合适。



