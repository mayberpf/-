# 神经网络的初始化方法总结
#### 初始化的原因
不正确初始化的权重，会导致梯度消失或爆炸问题，从而对训练过程产生负面影响。初始化权重过小，会导致收敛速度变慢，最坏的情况下，可能会阻止网络完全收敛。权重过大会导致前向传播或反向传播中梯度爆炸
#### 常见的初始化方法
##### 全零或等值初始化
由于初始化的值全部相同，每个神经元学到的东西也相同，将导致“对称性（Symmetry）”问题
##### 正态初始化
均值为零，标准差设置一个小值
好处：有相同偏差，权重有正有负
不适合非常深的网络
##### 均值初始化
##### Xavier initialization
初始化权重考虑了网络的大小（输入和输出单元的数量）。这种方法通过使权重与前一层中单元数的平方根成反比来确保权重保持在合理的值范围内。
Xavier 的初始化有两种变体：正态分布、均匀分布
适用于使用tanh 、sigmod为激活函数的网络
##### He initializtion
我可以理解为由于Xavier适用于tanh，但是对RELU等激活函数效果不好，因此提出He initialization。它有两种变体：正态分布、均匀分布。
适用于ReLU、Leaky ReLU等激活函数
##### Pre-trained
使用预训练的权重作为初始化，相比于其他初始化，收敛速度更快，起点更好。
##### 数据相关初始化
论文参考：https://arxiv.org/abs/1511.06856
##### 稀疏权重矩阵初始化
论文参考：https://openai.com/blog/block-sparse-gpu-kernels
##### 随机正交矩阵初始化
论文参考：https://arxiv.org/abs/1312.6120
