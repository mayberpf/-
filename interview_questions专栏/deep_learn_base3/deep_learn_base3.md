# 29
# 如何理解正则化
正则化是机器学习中的概念，它表示的是对损失函数增加额外的约束，来防止过拟合和提高模型泛化性，也就是运来的损失函数变成了原始损失函数+额外约束。在深度学习中呢，正则化常用来防止模型过拟合，通过对其施加了一些参数的约束，从而保证模型的泛化行。
L1正则化具有稀疏模型的能力。这个如何解释呢？
我们直到神经网络就是提取特征然后进行检测，那么如果过拟合可能是由于特征过拟合，也就是模型学到了我们不希望它学到的东西。比如之前在yolo中了解到的，关于飞机的识别模型可能会获得背景特征进行识别，这就不是很好。所以L1正则化实际上就是会将模型的一些权重变为0，这样来避免过拟合的发生。
还有一个说法，就是从数学方面进行解释，那么就是对于点集的分类，训练出一个高阶多项式，这个多项式中肯定系数越高会导致曲线越扭曲，越会发生过拟合，那么L1正则化，就负责将系数高的多项式的系数置为0。

# 正则化的方法有哪些
正则化包括：L1正则化、L2正则化、Dropout、Dropconnect、数据增强、多任务学习等
前面几个很容易理解，那就是防止过拟合的解决方法。那么为什么多任务学习也算正则化呢？这个有待思考。

# 如何理解L1和L2正则化
L1正则化是在正常的损失值后增加一项对权重取绝对值，使得模型在训练过程中，自主降低权重值，最终产生大量的0权值，变成一个稀疏模型
稀疏模型常常跟特征选择联系在一起，这其实很好理解，0权值的产生是模型训练产生的结果，它就有自主性，跟任务有关，跟输入的数据有关，灵活自主的根据任务和数据学习数据上的特征即选择特征。
上面提到的使得那些提取无关特征、偶然特征的卷积权值为0，就是一个选择特征的过程，即选择了跟任务有关的特征。
这就很好的解释了多任务学习算正则化的原因。
L2正则化是对权重做平方再取均值，其也称之为权重衰减
L2不会产生大量的0权值，而是接近于0的权值。因为权值平方求导，梯度是会随着权值的变化而改变的。