# 14题
### 一般来说大家都习惯用resnet或vgg或moblienet或CSPDarkNet这种现成的网络结构，相对较少地去设计一个模块
### 1、设计卷积神经网络的参数有哪些经验？（如卷积核大小、通道等）
### 2、设计卷积神经网络的结构有哪些经验？（如是否使用Dropout，skipconnect，什么情况用池化，什么情况用什么激活等）
### 3、设计一个特征金字塔的信息融合有哪些经验（一般的融合方法是什么，融合需要考虑哪些因素）
### 4、设计一个注意力模块有哪些经验？
### 5、损失函数的选择和使用有什么经验？（例如交叉熵损失前不能加softmax之类的经验）

##### 1
使用更大的通道数channe，理论上效果会更好
可以用多个卷积核来替代一个大卷积核，比如两个3 * 3代替一个5 * 5，这样可以起到减少计算量的作用，以及引入非线性效果
除了第一层使用比较大的卷积核，如7* 7，其他层一般使用较小的卷积核，如3 * 3 、1 * 1
使用1 * 1卷积核来做通道数的改变
在下采样时，一般会进行通道数的翻倍
使用stride = 2的卷积来代替池化

##### 2
如果要设计小网络，可以考虑使用深度可分离卷积、分组卷积来代替常规的卷积
如果想增大感受野，可以使用空洞卷积
在设计比较深的网络时，可以使用残差网络resnet结构
采用注意力机制，如SE
在检测、分割任务中，可以选择使用FPN来加强特征融合
可以使用BN层后可以考虑训练的收敛
使用了BN层之后可以考虑不适用dropout
对于二分类，可以使用sigmod激活函数，对于多分类则可以选择softmax
对于多标签分类任务，可以使用sigmod，为每一个类输出一个概率值
对于MLP和CNN网络，可以考虑默认使用ReLU
对于RNN网络，对隐藏层使用sigmod或tanh函数。其中tanh会有更好的效果

##### 3
信息的融合有concat和add两种方式，使用concat方式会有更多的特征
对于融合后的上采样，如FPN，可以使用转置卷积或者插值的方法，其中使用转置卷积的话，就会有更多的参数可以学习
如果你想进行双向的特征融合，可以结合PANet+FPN以及使用BiFPN

##### 4
即插即用的话，选择软注意力机制，如空间域、通道域注意力。（这里提一下软注意力与硬注意力的区别：硬注意力是不可微的注意，训练过程中往往加入增强学习，而软注意力是可微的）
如果想减少对外部信息的依赖，那么可以使用自注意力机制，注意力机制会更擅长捕捉特征的内部相关性，自注意力一般不用sigmod
较深的层一般不使用注意力，因为通道数过多，容易过拟合

（这个模块真不太懂）

##### 5
对于二分类任务，可以使用二元交叉熵BCE
对于多分类任务，可以使用分类交叉熵CE
对于多标签任务，可以使用二元交叉熵BCE
对于回归任务，可以使用均方差误差MSE
可以考虑使用smooth L1 loss代替L1 loss
对于多任务的训练，如YOLO，DBnet，不同的损失函数要赋予不同的系数，这里要进行调参，以此到达正常的收敛

个人感觉，这里所提到的点都是需要我们认真思考的！