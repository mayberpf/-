# Focal_loss
在之前，one-stage不如two-stage主要是因为样本不均衡原因----也就是正负样本不均衡和难易样本不均衡。
也就是：1、针对所有的负样本，因为基数过大，所以造成他的loss太大，以至于主导了损失函数，不利于收敛。2、针对单个负样本磊说，大多数的负样本不再前景和背景的过渡区域上，也就是说可以轻松的进行分类，这类可以归为easy-case，训练时对应的背景类的分数会很大，换句话说就是它所提供的loss很小，反向计算时梯度也会很小，梯度小的话，就会对模型的收敛起不到作用。因此这里提出了focal loss。
我的理解就是，举个例子，最近实际上我在做车道线的分割这件事情，在我使用的模型中，在训练过程中，我发现一个问题，那就是模型在训练的初始阶段，得到的特征图全部计算为了0。但是这并不妨碍loss函数值的下降，什么正确率也会有一定程度的上升。甚至于说，好像，还没有学习到很好的车道线分割能力，只是将结果的所有值都置为0，这就足够使bceloss下降很多，甚至降到0.0几以下。，接下来，我可能会在bceloss中添加focal loss的模块进行测试。
接下来我们来看一下focal loss主要是怎么实现的。
针对celoss的基本公式
@import "celoss.png"
那么这里实际上需要减少负样本的占比，就需要加一个比例系数在loss的计算中。这样就构成了focal_loss
@import "focal_loss1.png"
论文中提到的是可以将这个α设置为样本比例的反数。
为了解决难易样本不均衡，导致loss被easy case所主导的问题，引入了gamma ，这个参数能够降低easy case的损失，最终focal loss的定义就是下面这个
@import "focal_loss2.png"
## 实践