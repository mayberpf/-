# conv introduction
和标题一样，这篇文章是对conv的一个介绍，但是不是你想象的那个普通卷积的介绍，因为在学习过程中，接触到了各种卷积，比如2d卷积、3d卷积、甚至1d卷积，再有就是其他的种类，dw卷积？pw卷积？转置卷积？膨胀卷积？再有就是3d检测中的稀疏卷积？普通稀疏卷积？子流形稀疏卷积？焦点稀疏卷积？所以为了更好的区分这些的区别，写了这篇文章。
### 普通卷积
这里的普通卷积也就是2D卷积，相信这里没啥说的，就是很简单的操作。也就是一个卷积核在一个feature map上做扫描。卷积核与feature map之间做的是积和的操作。
至于卷积之后特征图的大小计算，包括卷积核大小计算是有一个转换公式。就是一个公式，N = (W − F + 2P )/S+1，就不详细解释啦。
但是这里可能需要了解一个事情，之前有一个面试，一位面试官问到了一个事情：卷积核的个数好像是。所以这部分可能需要举例了解一下。
首先每个卷积核都具有长宽高三个维度，长宽就是我们人为指定的3 * 3之类的。高度也就是深度，就是当前的feature map相同。
那么你可能会好奇，我们现在假设一个输入为224 * 224 * 3的特征，经过一个3 * 3的卷积，转换到了112 * 112 * 64的特征。接下来我们简单说一下参数。首先卷积核是3 * 3的.
```ruby
inputs = torch.rand((3,640,640))
print(inputs.shape)
conv = torch.nn.Conv2d(3,64,3,2,1)
outputs = conv(inputs)
print(outputs.shape)
```
上面写个代码简单测试一下。然后我发现了在卷积过程中的一个向下取整的问题，因为在计算过程中，肯定会有(W − F + 2P )不能整除S的问题，所以这里做了实验，让思路更清晰。所以经过计算，卷积的步长也就是S=2，padding=1。接下来就是卷积核，这一部分这么理解。卷积核是三维的，每个2d卷积包括了多个卷积核，包括多少卷积核的数量取决于输出的通道数。这样就会很好理解。卷积核的深度取决于输入特征的通道数，每个卷积核对特征图操作只会产生一个一层的特征图，所以输出64通道就需要64个卷积核。于是整个卷积过程需要的参数量也就迎刃而解啦，64个3 * 3 * 3的卷积核的参数量。
### 转置卷积
转置卷积又叫反卷积，看名字就清楚，但是转置卷积不是卷积的逆运算，因为普通卷积是不可逆的，只能恢复到原来的大小。也就是卷积多实现的是下采样过程，但是转置卷积实现的上采样过程。然后计算公式可以利用普通卷积的公式进行逆运算，easy：(W-1) * S-2P+k = N。
整个计算过程呢，我们设定的S步长和Padding均是对输入特征图进行的操作。下面举个例子：输入特征图大小2 * 2，输入输出均为单通道，通过转置卷积后得到4 * 4大小的图。转置卷积核k=3，s=1，p=0。
首先在元素件填充s-1行，然后在特征图四周填充k-p-1行，卷积核参数进行上下左右翻转，最后做正常卷积。
### 膨胀卷积
膨胀卷积又叫空洞卷积、扩张卷积。膨胀卷积是在标准卷积的基础上注入空洞，以此来增加感受野。所以膨胀卷积只是比普通卷积多了一个膨胀率的超参数，这个参数指的是卷积核的间隔数量。膨胀卷积是为解决语义分割任务提出的。
公式：W = (W-[k+(k-1)(a-1)]+2p)/s +1这里的a表示膨胀率
所有人都在说这里的膨胀卷积能增加感受野，但是我个人感觉，不是真正的增加感受野，因为中间很多地方都填充的0。
### 分组卷积
在特征图进行卷积的时候，首先对特征图分组，然后在进行卷积。
如何进行分组，只是在普通卷积中加一个超参进行说明，也就是group。
### 深度可分离卷积
深度可分离卷积是由DW卷积和PW卷积组成的。
DW卷积又叫逐通道卷积，就和名字一样，一个卷积核负责一个通道，一个通道只被一个卷积核卷积。所以，DW卷积之后的feature map的通道数和输入是一样的，这个是改变不了的。
代码的实现过程就是：利用分组卷积的操作，将每个通道进行分组，然后操作。
```ruby
#kernel_size=3, stride=1, dilation=1
depthwise = nn.Conv2d(inchannel, inchannel, kernel_size,stride=stride, padding=dilation,dilation=dilation, groups=inchannel, bias=bias)
```
PW卷积和普通卷积的运算非常相似，它的卷积核尺寸为1 * 1 * M，这里M是上一层的通道数。这里我个人感觉，这不就是1 * 1卷积嘛，没啥区别呀
```ruby
pointwise = nn.Conv2d(inplanes, planes, 1, bias=bias)
```
所以如果将上面两个进行拼接，就发现，实际上两步完成一个普通卷积的操作，同时将参数量减少。也就是通道和区域分开考虑。这种卷积多用于一些轻量级网络。
### 可变形卷积
可变形卷积在卷积核的每个采样点添加了一个可学习的偏移量offset。
这里建议去看下源码。
### 3D卷积
为什么这里要放这个，因为我发现在学习点云目标检测的过程中，会有3D卷积的操作，但是对这个操作的计算，不太熟悉。
但是3D卷积一定是类似与2D卷积，也就是一个卷积核会输出一个3D数组呗，输出的通道数等于卷积核的个数。输入的通道数等于卷积核的第四个维度？
当我查资料学习的时候，发现，3D卷积大多都用于处理视频数据。是的。我们先把参数放进来把
```ruby
torch.nn.Conv3D(c_in, c_out, kernel_size, stride, padding)
#和2D基本没有什么区别
```
3D卷积的输入数据shape为：(N,Cin,D,H,W)，输出size是(N,Cout,Dout,Hout,Wout)，其中N为训练的样本数，Cin为输入通道数，D这个参数就是3d比2d多的那一个维度的信息，H、W就不同多说了，输出与输入没差。接下来是我的个人观点，因为这部分要反映到点云的目标检测算法中，举个例子，在voxelnet中。有一个输入是(1,64,5,400,352)，这里1为bs，64位通道数，5应该就是每个体素中点的特征，400 * 352 不就是体素的H方向个数W方向个数。
```ruby
import torch
import torch.nn as nn
from torch import autograd
# kernel_size的第哥一维度的值是每次处理的图像帧数，后面是卷积核的大小
m = nn.Conv3d(3, 3, (3, 7, 7), stride=1, padding=0)
input = autograd.Variable(torch.randn(1, 3, 7, 60, 40))
output = m(input)
print(output.size())
# 输出是 torch.Size([1, 3, 5, 54, 34])
```
计算：
7-3+1=5
(60-7+0)/1+1=54
(40-7+0)/1+1=34
### 稀疏卷积
稀疏卷积是在接触到点云检测时涉及到的，因为点云和图像最大的区别就是点云是稀疏的，所以一般卷积就会差很多。因此出现了稀疏卷积，稀疏卷积和普通卷积的差别主要在输出方式。稀疏卷积具备两种输出方式：regular output definition，这种和普通卷积没有区别，另一种是submanifold output definition，这种输出方式是：只有当核中心覆盖输入站点时，才会计算卷积输出。
具体的计算方式这里就不赘述啦，用两张图就能简单的说明了。
@import "inputs.png"
@import "outputs.png"