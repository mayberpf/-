# 特征图可视化
在这篇文章中前言部分的吐槽，感觉和我很像，但是确实是一语道破！
因为我们都知道模型好不好，我直接将结果可视化就好了呀，为什么要看中间的特征图呢？不过中间特征图可能也是有用的。
这里我简单概括前言的意思：特征图主要是在论文中凑字数，只要可以合理解释！所以还是很重要的。
接下来我们来说说这一部分怎么完成。
## 初始化配置
其中主要包括初始化，加载数据，修改网络，定义网络，加载预训练模型。
前面的部分，数据读取我们就不说了，和推理过程中的数据读取可以理解为是基本一样的。接下来就是网络模型的更改，实际上特征图可视化可以理解将数据放入网络，我们需要将网络中的feature map输出，然后将其可视化。那么网络的更改就简单了，实际上就是在前向传播的过程中，将想输出的特征图的名字改一下，然后记录下来最后return返回即可。
```ruby
def forward(self, x):
    x = self.model.conv1(x)
    x = self.model.bn1(x)
    x = self.model.relu(x)
    x = self.model.maxpool(x)
    feature = self.model.layer1(x)
    x = self.model.layer2(feature)
    x = self.model.layer3(x)
    x = self.model.layer4(x)
    return feature,x
```
接下里就和验证环节没有什么区别，加载模型，加载预训练模型（我个人感觉这里加载预训练模型不太对，实际上应该是加载训练好的权重文件）。但是需要注意的是：一定要设置模型是验证的模式。
```ruby
model = model.cuda().eval()
```
拿到了feature map的数据之后，我想就是可视化了，那么这就和在训练过程中让验证结果可视化没什么区别。但是需要注意一点！还是有一点区别的，因为我们平时在做分割时，最后结果的通道数一般为1或者3。但是中间的feature map的通道数可以是很大的。所以这里我们需要做的就是提取feature map的每个通道的数据，然后将其可视化。
这里有一个代码，做的就是特征图可视化的代码，可以参考：
```ruby
def visualize_feature_map(img_batch,out_path,type,BI):
    feature_map = torch.squeeze(img_batch)
    feature_map = feature_map.detach().cpu().numpy()
    feature_map_sum = feature_map[0, :, :]
    feature_map_sum = np.expand_dims(feature_map_sum, axis=2)
    for i in range(0, 2048):
        feature_map_split = feature_map[i,:, :]
        feature_map_split = np.expand_dims(feature_map_split,axis=2)
        if i > 0:
            feature_map_sum +=feature_map_split
        feature_map_split = BI.transform(feature_map_split)
        plt.imshow(feature_map_split)
        plt.savefig(out_path + str(i) + "_{}.jpg".format(type) )
        plt.xticks()
        plt.yticks()
        plt.axis('off')
    feature_map_sum = BI.transform(feature_map_sum)
    plt.imshow(feature_map_sum)
    plt.savefig(out_path + "sum_{}.jpg".format(type))
    print("save sum_{}.jpg".format(type))
```
在这个代码里面有一个函数BI，这是一个双线性插值，主要解决，在中间过程中，图片经过下采样，特征图大小很小，也就是将特征图放大的函数
```ruby
class BilinearInterpolation(object):
    def __init__(self, w_rate: float, h_rate: float, *, align='center'):
        if align not in ['center', 'left']:
            logging.exception(f'{align} is not a valid align parameter')
            align = 'center'
        self.align = align
        self.w_rate = w_rate
        self.h_rate = h_rate
    def set_rate(self,w_rate: float, h_rate: float):
        self.w_rate = w_rate # w 的缩放率
        self.h_rate = h_rate # h 的缩放率
        # 由变换后的像素坐标得到原图像的坐标       针对高
    def get_src_h(self, dst_i,source_h,goal_h) -> float:
        if self.align == 'left':
        # 左上角对齐
            src_i = float(dst_i * (source_h/goal_h))
        elif self.align == 'center':
        # 将两个图像的几何中心重合。
            src_i = float((dst_i + 0.5) * (source_h/goal_h) - 0.5)
        src_i += 0.001
        src_i = max(0.0, src_i)
        src_i = min(float(source_h - 1), src_i)
        return src_i
    # 由变换后的像素坐标得到原图像的坐标
    针对宽
    def get_src_w(self, dst_j,source_w,goal_w) -> float:
        if self.align == 'left':
        # 左上角对齐
            src_j = float(dst_j * (source_w/goal_w))
        elif self.align == 'center':
        # 将两个图像的几何中心重合。
            src_j = float((dst_j + 0.5) * (source_w/goal_w) - 0.5)
        src_j += 0.001
        src_j = max(0.0, src_j)
        src_j = min((source_w - 1), src_j)
        return src_j
    def transform(self, img):
        source_h, source_w, source_c = img.shape
        # (235, 234, 3)
        goal_h, goal_w = round(source_h * self.h_rate), round(source_w * self.w_rate)
        new_img = np.zeros((goal_h, goal_w, source_c), dtype=np.uint8)
        for i in range(new_img.shape[0]):
            # h
            src_i = self.get_src_h(i,source_h,goal_h)
            for j in range(new_img.shape[1]):
                src_j = self.get_src_w(j,source_w,goal_w)
                i2 = ceil(src_i)
                i1 = int(src_i)
                j2 = ceil(src_j)
                j1 = int(src_j)
                x2_x = j2 - src_j
                x_x1 = src_j - j1
                y2_y = i2 - src_i
                y_y1 = src_i - i1
                new_img[i, j] = img[i1, j1]*x2_x*y2_y + img[i1, j2] * \
                x_x1*y2_y + img[i2, j1]*x2_x*y_y1 + img[i2, j2]*x_x1*y_y1
        return new_img
    #使用方法
    BI = BilinearInterpolation(8, 8)
    feature_map = BI.transform(feature_map)
```

最后main函数的操作流程
```ruby
imgs_path = "/path/to/imgs/"
save_path = "/save/path/to/output/"
model = Init_Setting(120)
BI = BilinearInterpolation(8, 8)
data = image_proprecess(out_path + "0836.jpg")
data = data.cuda()
output, _ = model(data)
visualize_feature_map(output, save_path, "drone", BI)
```