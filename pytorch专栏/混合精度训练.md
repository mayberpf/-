# 混合精度训练
第一次介绍这一部分实际上就是在那个比赛中。已经算是会写了这部分的代码。其实使用起来还是很简单的。
这里使用的就是torch.amp的库。关于混合精度训练呢，实际上就是在之前我们训练时数据类型都是FP32的格式，现在是FP32和FP16混合使用，也就是我将某些数据的精度改小，这就会节省出一部分显存并且加快训练的速度。但是需要注意的是pytorch的版本问题，在pytorch1.6后的版本，将amp集成在了cuda版本中。
FP32指数占8位，尾数占23位，数据的动态范围是[2^-126,2^127]是深度学习框架训练时常用的数据类型
FP16指数占5位，尾数占10位，相比FP32，FP16的表示范围更窄，最小可表示的正数数值为2^-14，最大为65504,容易数据溢出。
# 使用
首先我们还是介绍这个东西如何使用。使用的是autocast，这个用于改变上下文张量数据类型转变为半精度浮点型。
注意：autocast只包含网络的前向传播过程以及loss的计算，并不包含反向传播。因为反向传播的op会使用和前向传播op一样的类型。
```ruby
from torch.cuda.amp import autocast as autocast
model=Net().cuda()
optimizer=optim.SGD(model.parameters(),...)
for input,target in data:
    optimizer.zero_grad()
    with autocast():
        output=model(input)
        loss = loss_fn(output,target)
    loss.backward()
    optimizer.step()
```
转换过后数据类型，还需要设置一个转换loss的函数，保证loss在反向传播的过程中，尽可能减少梯度的下溢出和上溢出，就是通过控制scaler的大小。
```ruby
from torch.cuda.amp import autocast as autocast
model=Net().cuda()
optimizer=optim.SGD(model.parameters(),...)
scaler = GradScaler() # 训练前先实例化出一个GradScaler对象
for epoch in epochs:
    for input,target in data:
        optimizer.zero_grad()
        # 正常更新权重
        with autocast(): # 开启autocast上下文,在autocast上下文范围内,进行模型的前向推理和loss计算
            output=model(input)
            loss = loss_fn(output,targt)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
    # 对loss进行放大,针对放大后的loss进行反向传播
    # 在这里,首先会把梯度值缩放回来,如果缩放后的梯度不是inf或NaN,那么就会调用optimizer.step()来更新权重,否则,忽略step调用,从而保证权重不更新。
        scaler.update()
    # 看是否要增大scaler,更新scalar的缩放信息
```