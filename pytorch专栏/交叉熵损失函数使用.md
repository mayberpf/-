# 关于交叉熵损失函数使用
### 交叉熵损失函数的理论
监督学习可以分为两类：分类和回归
分类又可以分为|二分类、多分类
交叉熵损失函数是分类器中最常用的损失函数，交叉熵用来度量两个概率分布的差异性，用来衡量模型学习到的分布和真实分布的差异
举个例子：在二分类的情况中，假设狗(1)的概率为P，猫(0)的概率为1-P，因此二分类交叉熵损失的一般形式为，其中y为标签
Loss = -(ylog P +(1-y)log(1-P))
我的理解是这个loss函数分为了2个部分，可以这样理解：公式的前半部分ylog P是针对分类正确但是概率不高的情况、公式的后半部分
(1-y)log(1-P)针对的是分类不正确的情况。
为了更好的理解交叉熵函数，需要理解的一些概念
##### 信息量，信息熵，相对熵，交叉熵
信息量大大小和事件发生的概率成反比
信息熵是在结果出来之前对可能产生的信息量的期待，期望可以理解为所有可能结果的概率乘以该对应的结果。
信息熵是用来衡量事物不确定性。信息熵越大，事物越具有不确定性。
相对熵，表示两个概率分布的 差异，当两个随机分布相同时，他们的 相对熵为零。
交叉熵。相对熵 = 交叉熵-信息熵
在机器学习和深度学习中，样本和标签已知，那么信息熵相当于常量，此时只需要拟合交叉熵即可。
#### 在pytorch中使用
```r
torch.nn.CrossEntropyLoss
#在pytorch中torch.nn.CrossEntropyLoss中是已经包含了softmax
```
所以在使用时，不能再进行softmax，否则做了两次softmax，相对于在正常的预测概率的基础上做了一个标签平滑化，导致模型训练速度非常缓慢，近似不收敛。
如果你代码不收敛，可以检查一下loss的使用是否正确。
使用方法
```r
loss = nn.CrossEntropLoss()
input = torch.randn(3,5,requires_grad = True)
target = torch.empty(3,dtype = torch.long).random_(5)
output = loss(input,target)
```
这里也就可以解释之前为什么要使用是one-hot编码，而不是用1，2，3label分类