# 模型并行+分布式训练
想总结这一部分的原因，主要是之前我记得torch.nn.DataParalle是在有多张GPU时才会使用的，但是最近看了一个代码，就是只有一个GPU，但是还是用到了torch.nn.DataParalle。模型中有这几行代码，我不太能理解
```ruby
        model_train = torch.nn.DataParallel(model)
        cudnn.benchmark = True
        model_train = model_train.cuda()
```
